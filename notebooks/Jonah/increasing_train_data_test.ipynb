{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "import pytorch_lightning.callbacks as pl_callbacks\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import eq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading existing catalog from /home/gcl/RA/jonahk/recast/data/ANSS_MultiCatalog.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "catalog = eq.catalogs.ANSS_MultiCatalog(    \n",
    "    num_sequences=3,\n",
    "    t_end_days=365,\n",
    "    mag_completeness=4.5,\n",
    "    minimum_mainshock_mag=6.0,\n",
    "    radius_kilometers = 1000,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Sequence(\n",
       "   inter_times: [492],\n",
       "   arrival_times: [491],\n",
       "   t_start: 0.0,\n",
       "   t_end: 365.0,\n",
       "   t_nll_start: 0.0,\n",
       "   mag: [491],\n",
       "   mag_bounds: [2],\n",
       "   mag_nll_bounds: [2],\n",
       "   extra_feat: [491]\n",
       " )]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "catalog.train.sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "InMemoryDataset(1)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "catalog.train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl_val = catalog.val.get_dataloader(batch_size=min(500,len(catalog.val)))\n",
    "dl_train = catalog.train.get_dataloader(batch_size=min(500,len(catalog.train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading existing catalog from /home/gcl/RA/jonahk/recast/data/ANSS_MultiCatalog.\n"
     ]
    }
   ],
   "source": [
    "catalog_exp = eq.catalogs.ANSS_MultiCatalog(    \n",
    "    num_sequences=18000,\n",
    "    t_end_days=365,\n",
    "    mag_completeness=4.5,\n",
    "    minimum_mainshock_mag=6.0,\n",
    "    radius_kilometers = 1000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequence(\n",
       "  inter_times: [112],\n",
       "  arrival_times: [111],\n",
       "  t_start: 0.0,\n",
       "  t_end: 365.0,\n",
       "  t_nll_start: 0.0,\n",
       "  mag: [111]\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "catalog_exp.train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/gcl/RA/jonahk/miniconda3/envs/DS-discovery/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/logger_connector/logger_connector.py:67: UserWarning: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `pytorch_lightning` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n",
      "  warning_cache.warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2]\n",
      "\n",
      "  | Name          | Type    | Params\n",
      "------------------------------------------\n",
      "0 | hypernet_time | Linear  | 3.2 K \n",
      "1 | hypernet_mag  | Linear  | 33    \n",
      "2 | rnn           | GRU     | 3.5 K \n",
      "3 | dropout       | Dropout | 0     \n",
      "------------------------------------------\n",
      "6.7 K     Trainable params\n",
      "0         Non-trainable params\n",
      "6.7 K     Total params\n",
      "0.027     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c95285b8c80347389e9d7456a61dd736",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gcl/RA/jonahk/miniconda3/envs/DS-discovery/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 64 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/gcl/RA/jonahk/miniconda3/envs/DS-discovery/lib/python3.11/site-packages/pytorch_lightning/utilities/data.py:103: UserWarning: Total length of `DataLoader` across ranks is zero. Please make sure this was your intention.\n",
      "  rank_zero_warn(\n",
      "/home/gcl/RA/jonahk/miniconda3/envs/DS-discovery/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 64 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b9739e0d7e34ccb869f61450c4ffb33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "Early stopping conditioned on metric `total_val_loss` which is not available. Pass in or modify your `EarlyStopping` callback to use any of the following: `train_loss`, `total_train_loss`",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 27\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[39m# Trainer set up training and validation loops with previous specs\u001b[39;00m\n\u001b[1;32m     25\u001b[0m trainer \u001b[39m=\u001b[39m pl\u001b[39m.\u001b[39mTrainer(devices\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,max_epochs\u001b[39m=\u001b[39m\u001b[39m10000\u001b[39m, callbacks\u001b[39m=\u001b[39m[checkpoint, early_stopping],log_every_n_steps\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m---> 27\u001b[0m trainer\u001b[39m.\u001b[39;49mfit(model, dl_train, dl_val)\n",
      "File \u001b[0;32m~/miniconda3/envs/DS-discovery/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py:532\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    530\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39m_lightning_module \u001b[39m=\u001b[39m model\n\u001b[1;32m    531\u001b[0m _verify_strategy_supports_compile(model, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstrategy)\n\u001b[0;32m--> 532\u001b[0m call\u001b[39m.\u001b[39;49m_call_and_handle_interrupt(\n\u001b[1;32m    533\u001b[0m     \u001b[39mself\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit_impl, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path\n\u001b[1;32m    534\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/DS-discovery/lib/python3.11/site-packages/pytorch_lightning/trainer/call.py:43\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[39mif\u001b[39;00m trainer\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39mlauncher \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     42\u001b[0m         \u001b[39mreturn\u001b[39;00m trainer\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39mlauncher\u001b[39m.\u001b[39mlaunch(trainer_fn, \u001b[39m*\u001b[39margs, trainer\u001b[39m=\u001b[39mtrainer, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m---> 43\u001b[0m     \u001b[39mreturn\u001b[39;00m trainer_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     45\u001b[0m \u001b[39mexcept\u001b[39;00m _TunerExitException:\n\u001b[1;32m     46\u001b[0m     _call_teardown_hook(trainer)\n",
      "File \u001b[0;32m~/miniconda3/envs/DS-discovery/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py:571\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    561\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_data_connector\u001b[39m.\u001b[39mattach_data(\n\u001b[1;32m    562\u001b[0m     model, train_dataloaders\u001b[39m=\u001b[39mtrain_dataloaders, val_dataloaders\u001b[39m=\u001b[39mval_dataloaders, datamodule\u001b[39m=\u001b[39mdatamodule\n\u001b[1;32m    563\u001b[0m )\n\u001b[1;32m    565\u001b[0m ckpt_path \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_checkpoint_connector\u001b[39m.\u001b[39m_select_ckpt_path(\n\u001b[1;32m    566\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mfn,\n\u001b[1;32m    567\u001b[0m     ckpt_path,\n\u001b[1;32m    568\u001b[0m     model_provided\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m    569\u001b[0m     model_connected\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlightning_module \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    570\u001b[0m )\n\u001b[0;32m--> 571\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run(model, ckpt_path\u001b[39m=\u001b[39;49mckpt_path)\n\u001b[1;32m    573\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mstopped\n\u001b[1;32m    574\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/DS-discovery/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py:980\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m    975\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_signal_connector\u001b[39m.\u001b[39mregister_signal_handlers()\n\u001b[1;32m    977\u001b[0m \u001b[39m# ----------------------------\u001b[39;00m\n\u001b[1;32m    978\u001b[0m \u001b[39m# RUN THE TRAINER\u001b[39;00m\n\u001b[1;32m    979\u001b[0m \u001b[39m# ----------------------------\u001b[39;00m\n\u001b[0;32m--> 980\u001b[0m results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_stage()\n\u001b[1;32m    982\u001b[0m \u001b[39m# ----------------------------\u001b[39;00m\n\u001b[1;32m    983\u001b[0m \u001b[39m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[1;32m    984\u001b[0m \u001b[39m# ----------------------------\u001b[39;00m\n\u001b[1;32m    985\u001b[0m log\u001b[39m.\u001b[39mdebug(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m: trainer tearing down\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/DS-discovery/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py:1023\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1021\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_run_sanity_check()\n\u001b[1;32m   1022\u001b[0m     \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mset_detect_anomaly(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_detect_anomaly):\n\u001b[0;32m-> 1023\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfit_loop\u001b[39m.\u001b[39;49mrun()\n\u001b[1;32m   1024\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1025\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUnexpected state \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/DS-discovery/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:203\u001b[0m, in \u001b[0;36m_FitLoop.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    201\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_start()\n\u001b[1;32m    202\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39madvance()\n\u001b[0;32m--> 203\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mon_advance_end()\n\u001b[1;32m    204\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_restarting \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m    205\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/DS-discovery/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:370\u001b[0m, in \u001b[0;36m_FitLoop.on_advance_end\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    368\u001b[0m call\u001b[39m.\u001b[39m_call_callback_hooks(trainer, \u001b[39m\"\u001b[39m\u001b[39mon_train_epoch_end\u001b[39m\u001b[39m\"\u001b[39m, monitoring_callbacks\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m    369\u001b[0m call\u001b[39m.\u001b[39m_call_lightning_module_hook(trainer, \u001b[39m\"\u001b[39m\u001b[39mon_train_epoch_end\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 370\u001b[0m call\u001b[39m.\u001b[39;49m_call_callback_hooks(trainer, \u001b[39m\"\u001b[39;49m\u001b[39mon_train_epoch_end\u001b[39;49m\u001b[39m\"\u001b[39;49m, monitoring_callbacks\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m    372\u001b[0m trainer\u001b[39m.\u001b[39m_logger_connector\u001b[39m.\u001b[39mon_epoch_end()\n\u001b[1;32m    374\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mepoch_loop\u001b[39m.\u001b[39m_num_ready_batches_reached():\n\u001b[1;32m    375\u001b[0m     \u001b[39m# if we are restarting and the above condition holds, it's because we are reloading an epoch-end checkpoint.\u001b[39;00m\n\u001b[1;32m    376\u001b[0m     \u001b[39m# since metric-based schedulers require access to metrics and those are not currently saved in the\u001b[39;00m\n\u001b[1;32m    377\u001b[0m     \u001b[39m# checkpoint, the plateau schedulers shouldn't be updated\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/DS-discovery/lib/python3.11/site-packages/pytorch_lightning/trainer/call.py:195\u001b[0m, in \u001b[0;36m_call_callback_hooks\u001b[0;34m(trainer, hook_name, monitoring_callbacks, *args, **kwargs)\u001b[0m\n\u001b[1;32m    193\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mcallable\u001b[39m(fn):\n\u001b[1;32m    194\u001b[0m         \u001b[39mwith\u001b[39;00m trainer\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mprofile(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m[Callback]\u001b[39m\u001b[39m{\u001b[39;00mcallback\u001b[39m.\u001b[39mstate_key\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m{\u001b[39;00mhook_name\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m):\n\u001b[0;32m--> 195\u001b[0m             fn(trainer, trainer\u001b[39m.\u001b[39;49mlightning_module, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    197\u001b[0m \u001b[39mif\u001b[39;00m pl_module:\n\u001b[1;32m    198\u001b[0m     \u001b[39m# restore current_fx when nested context\u001b[39;00m\n\u001b[1;32m    199\u001b[0m     pl_module\u001b[39m.\u001b[39m_current_fx_name \u001b[39m=\u001b[39m prev_fx_name\n",
      "File \u001b[0;32m~/miniconda3/envs/DS-discovery/lib/python3.11/site-packages/pytorch_lightning/callbacks/early_stopping.py:182\u001b[0m, in \u001b[0;36mEarlyStopping.on_train_epoch_end\u001b[0;34m(self, trainer, pl_module)\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_on_train_epoch_end \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_should_skip_check(trainer):\n\u001b[1;32m    181\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[0;32m--> 182\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_early_stopping_check(trainer)\n",
      "File \u001b[0;32m~/miniconda3/envs/DS-discovery/lib/python3.11/site-packages/pytorch_lightning/callbacks/early_stopping.py:193\u001b[0m, in \u001b[0;36mEarlyStopping._run_early_stopping_check\u001b[0;34m(self, trainer)\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Checks whether the early stopping condition is met and if so tells the trainer to stop the training.\"\"\"\u001b[39;00m\n\u001b[1;32m    191\u001b[0m logs \u001b[39m=\u001b[39m trainer\u001b[39m.\u001b[39mcallback_metrics\n\u001b[0;32m--> 193\u001b[0m \u001b[39mif\u001b[39;00m trainer\u001b[39m.\u001b[39mfast_dev_run \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_condition_metric(  \u001b[39m# disable early_stopping with fast_dev_run\u001b[39;49;00m\n\u001b[1;32m    194\u001b[0m     logs\n\u001b[1;32m    195\u001b[0m ):  \u001b[39m# short circuit if metric not present\u001b[39;00m\n\u001b[1;32m    196\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[1;32m    198\u001b[0m current \u001b[39m=\u001b[39m logs[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmonitor]\u001b[39m.\u001b[39msqueeze()\n",
      "File \u001b[0;32m~/miniconda3/envs/DS-discovery/lib/python3.11/site-packages/pytorch_lightning/callbacks/early_stopping.py:148\u001b[0m, in \u001b[0;36mEarlyStopping._validate_condition_metric\u001b[0;34m(self, logs)\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[39mif\u001b[39;00m monitor_val \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    147\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstrict:\n\u001b[0;32m--> 148\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(error_msg)\n\u001b[1;32m    149\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    150\u001b[0m         rank_zero_warn(error_msg, category\u001b[39m=\u001b[39m\u001b[39mRuntimeWarning\u001b[39;00m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Early stopping conditioned on metric `total_val_loss` which is not available. Pass in or modify your `EarlyStopping` callback to use any of the following: `train_loss`, `total_train_loss`"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "dl_train = catalog.train.get_dataloader(batch_size=1)\n",
    "dl_val = catalog.val.get_dataloader(batch_size=1)\n",
    "dl_test = catalog.test.get_dataloader(batch_size=1)\n",
    "\n",
    "T = catalog.train[0].t_end\n",
    "N = np.mean([len(seq) for seq in catalog.train])\n",
    "mag_mean = np.mean([catalog.train[0].mag.mean().item() for seq in catalog.train])\n",
    "tau_mean = T/N\n",
    "\n",
    "model = eq.models.RecurrentTPP(\n",
    "    tau_mean = tau_mean,\n",
    "    learning_rate=1e-3,\n",
    ")\n",
    "\n",
    "    # ModelCheckpoints saves the model with the best validation loss\n",
    "checkpoint = pl_callbacks.ModelCheckpoint(monitor=\"total_val_loss\")\n",
    "\n",
    "    # EarlyStopping stops training if the validation loss doesn't improve by more than 1e-3 for 20 epochs\n",
    "early_stopping = pl_callbacks.EarlyStopping(monitor=\"total_val_loss\", patience=10, min_delta=1e-5)\n",
    "\n",
    "    # RichProgressBar adds a nice and more functional progress bar\n",
    "progress_bar = pl_callbacks.RichProgressBar()\n",
    "\n",
    "    # Trainer set up training and validation loops with previous specs\n",
    "trainer = pl.Trainer(devices=1,max_epochs=10000, callbacks=[checkpoint, early_stopping],log_every_n_steps=1)\n",
    "\n",
    "trainer.fit(model, dl_train, dl_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RecurrentTPP(\n",
       "  (hypernet_time): Linear(in_features=32, out_features=96, bias=True)\n",
       "  (hypernet_mag): Linear(in_features=32, out_features=1, bias=True)\n",
       "  (rnn): GRU(2, 32, batch_first=True)\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint.best_model_path\n",
    "model.load_from_checkpoint(checkpoint.best_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2]\n",
      "/home/gcl/RA/jonahk/miniconda3/envs/DS-discovery/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:442: PossibleUserWarning: The dataloader, test_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 64 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a67751cabe6545ad899e4dcd16d0a53d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">   Runningstage.testing    </span>┃<span style=\"font-weight: bold\">                           </span>┃\n",
       "┃<span style=\"font-weight: bold\">          metric           </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test_loss         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    -2.942216157913208     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">      total_test_loss      </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    -2.942216157913208     </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m  Runningstage.testing   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m                           \u001b[0m┃\n",
       "┃\u001b[1m \u001b[0m\u001b[1m         metric          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test_loss        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   -2.942216157913208    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m     total_test_loss     \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   -2.942216157913208    \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'test_loss': -2.942216157913208, 'total_test_loss': -2.942216157913208}]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.test(model, dl_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DS-discovery",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
